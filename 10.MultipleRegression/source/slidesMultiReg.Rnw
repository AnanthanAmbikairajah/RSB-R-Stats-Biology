\documentclass[10pt]{beamer}%
\setbeamersize{text margin left=0.5cm, text margin right=0.5cm}

\usepackage{alltt}%
\usetheme[background=light]{metropolis} 
\usecolortheme{seahorse}

\usepackage[utf8]{inputenc}%


\usepackage[normalem]{ulem}%strikeout
 

% graphics
%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{xcolor}%for color mixing

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}

\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% Doc info %%%%%%%%%%%%%%%%%%%
\title{Multiple regressions}
\author{Timoth\'ee Bonnet}
\institute{BDSI / RSB}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

<<Plot Options, echo=FALSE, message=FALSE>>=
#load(file = ".RData")
opts_knit$set(width=60)
opts_chunk$set(comment=NA, fig.width=8, fig.height=6, out.width='0.8\\textwidth',
               out.height='0.6\\textwidth',background='#D7DDEB', size="small")


szgr <- 2
szax <- 1.3
marr <- c(4, 4, 1, 1) + 0.1
setPar<-function(){
par(las=1,mar=marr, cex=szgr, cex.lab=szax , cex.axis=szax, lwd=2 ,pch=1, las=1)
}
setPar()
@

\begin{frame}
  \begin{quote}
  ``Simple and multiple regression are two of the most-used statistical procedures in biology. Statistical results from both procedures are commonly interpreted as metrics of the degree of relationship between (sometimes multiple) explanatory and response variables. This rough interpretation may generally be satisfactory for simple regressions, i.e., models involving only one explanatory variable. However, this interpretation can lead to confusion for multiple regression, where the coefficients of a multiple regression model measure something subtly but crucially different\dots"
  \end{quote}
Morrissey \& Ruxton (2018) \textbf{Multiple Regression Is Not Multiple Regressions: The Meaning of Multiple Regression and the Non-Problem of Collinearity}. PTPBIO 10(3)

\end{frame}
%%%%%%%%%%%

\begin{frame}
\maketitle
\end{frame}
%%%%%%%%%%%


\AtBeginSection[]
{
  \begin{frame}<beamer>
    \Large
    \frametitle{}
    \tableofcontents[currentsection,hideothersubsections,subsectionstyle=hide]% down vote\tableofcontents[currentsection,currentsubsection,hideothersubsections,sectionstyle=show/hide,subsectionstyle=show/shaded/hide]
  \end{frame}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear model, reminder}

\begin{frame}[fragile]{A simple linear model}
  \textbf{{\color{purple}{Response}} = {\color{blue}{Intercept}} + {\color{red}{Slope}} $\times$ {\color{orange}{Predictor}} + {\color{gray}{Error}}} \\

  <<lmprinc0, echo=FALSE, dev='tikz'>>=
    setPar()
  set.seed(123)
    x <- rnorm(20)
    y <- 1 + x + rnorm(20)
    
    plot(x, y, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}")
    lm0 <- lm(y~x)
    abline(lm0, col="red", lwd=5)
    abline(h=coef(lm0)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)

    arrows(x0 = x, y0=y, y1=lm0$fitted.values, code=0, col="gray", lwd=3)
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{A multiple linear model}
  \textbf{{\color{purple}{Response}} = {\color{blue}{Intercept}} + {\color{red}{Slope1}} $\times$ {\color{orange}{Predictor1}} + {\color{red}{Slope2}} $\times$ {\color{orange}{Predictor2}}  + {\color{gray}{Error}}} \\
  \vspace{1cm}
\textbf{In R:}
<<eval=FALSE>>=
  lm(response ~ 1 + predictor1 + predictor2, data=data)
@


\end{frame}
%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple regression}

\begin{frame}[fragile]{Sequential regression}

    <<echo=FALSE>>=
    set.seed(123)
    x1 <- rnorm(n = 20)
    x2 <- rnorm(n=20) + 0.6* x1
    x3 <- rnorm(n=20) - 0.5* x1
    y <- 0.5 +x1 + x2 + 0.1*x3 +rnorm(20, 0, 0.4)
    m1 <- lm(y ~ x1)
    m2 <- lm(m1$residuals ~ x2)
    m3 <- lm(m2$residuals ~ x3)
  @
  
We want to explain a response by three predictors
\only<2>{
  <<lmprinc1, echo=FALSE, dev='tikz'>>=
    setPar()
    plot(x1, y, xlab="\\color{orange}{Predictor 1}", ylab="\\color{purple}{Response}")
    abline(m1, col="red", lwd=5)
    abline(h=coef(m1)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)
    arrows(x0 = x1, y0=y, y1=m1$fitted.values, code=0, col="gray", lwd=3)
  @
}
\only<3>{
  <<lmprinc2, echo=FALSE, dev='tikz'>>=
    setPar()
      plot(x2, m1$residuals, xlab="\\color{orange}{Predictor 2}", ylab="\\color{purple}{Residuals from 1}")
    abline(m2, col="red", lwd=5)
    abline(h=coef(m2)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)
    arrows(x0 = x2, y0=m1$residuals, y1=m2$fitted.values, code=0, col="gray", lwd=3)
  @
}
\only<4>{
  <<lmprinc3, echo=FALSE, dev='tikz'>>=
    setPar()
      plot(x3, m2$residuals, xlab="\\color{orange}{Predictor 3}", ylab="\\color{purple}{Residuals from 2}")
    abline(m3, col="red", lwd=5)
    abline(h=coef(m3)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)
    arrows(x0 = x3, y0=m2$residuals, y1=m3$fitted.values, code=0, col="gray", lwd=3)
  @
}

\end{frame}
%%%%%%%%%%%



\begin{frame}[fragile]{Sequential regression}

  <<>>=
    m1 <- lm(y ~ x1)
    m2 <- lm(m1$residuals ~ x2)
    m3 <- lm(m2$residuals ~ x3)
    @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Sequential regression}

But estimates in
  <<>>=
    m1 <- lm(y ~ x1)
    m2 <- lm(m1$residuals ~ x2)
    m3 <- lm(m2$residuals ~ x3)
    c(coefficients(m1)[2], coefficients(m2)[2], coefficients(m3)[2])
    @
are different from
<<>>=
    m1 <- lm(y ~ x3)
    m2 <- lm(m1$residuals ~ x2)
    m3 <- lm(m2$residuals ~ x1)
    c(coefficients(m1)[2], coefficients(m2)[2], coefficients(m3)[2])
    @
\end{frame}
%%%%%%%%%%%
\begin{frame}[fragile]{Sequential regression}
  Also what happens with classical ANOVA (aov in R)
  <<>>=
    summary(aov(y ~ x1 + x2 + x3))
    summary(aov(y ~ x2 + x3 + x1))
    @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Multiple regression}
In contrast lm() optimizes relationships simultaneously\\
Order does \textbf{not} matter:
  <<>>=
    coefficients(lm(y ~ x1 + x2 + x3))
    coefficients(lm(y ~ x2 + x3 + x1))
  @

\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Multiple regression}
\textbf{BUT} estimates may change with extra covariates
<<>>=
    coefficients(lm(y ~ x1 + x2 ))
    coefficients(lm(y ~ x1 + x2 + x3))
  @
\pause
\begin{block}{??}
  \begin{itemize}
    \item That is a good thing
    \item Estimates are independent effects, conditional on the other parameters
  \end{itemize}
\end{block}
\end{frame}
%%%%%%%%%%%



\begin{frame}[fragile]{Multiple regression}
<<eval=FALSE>>=
        library("plot3D")
fit <- lm(y ~ x1 + x2)
# predict values on regular xy grid
grid.lines = 26
x1.pred <- seq(min(x1), max(x1), length.out = grid.lines)
x2.pred <- seq(min(x2), max(x2), length.out = grid.lines)
x1x2 <- expand.grid( x1 = x1.pred, x2 = x2.pred)
y.pred <- matrix(predict(fit, newdata = x1x2), 
                 nrow = grid.lines, ncol = grid.lines)
fitpoints <- predict(fit)
scatter3D(x1, x3, y, pch = 18, cex = 2, 
    theta =18, phi = -18, ticktype = "detailed",
    xlab = "x1", ylab = "x2", zlab = "y",  
    surf = list(x = x1.pred, y = x2.pred, z = y.pred,  
    facets = NA, fit = fitpoints), main = "")   
@
    \end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Multiple regression}

<<echo=FALSE, dev='tikz'>>=
library("plot3D")
setPar()
fit <- lm(y ~ x1 + x2)
# predict values on regular xy grid
grid.lines = 26
x1.pred <- seq(min(x1), max(x1), length.out = grid.lines)
x2.pred <- seq(min(x2), max(x2), length.out = grid.lines)
x1x2 <- expand.grid( x1 = x1.pred, x2 = x2.pred)
y.pred <- matrix(predict(fit, newdata = x1x2), 
                 nrow = grid.lines, ncol = grid.lines)
fitpoints <- predict(fit)
scatter3D(x1, x2, y, pch = 18, cex = 2, 
    theta =20, phi = 18, ticktype = "detailed",
    xlab = "x1", ylab = "x2", zlab = "y",  
    surf = list(x = x1.pred, y = x2.pred, z = y.pred,  
    facets = NA, fit = fitpoints), main = "")   
@
    \end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Multiple regression}

<<eval=FALSE>>=
    library("plot3Drgl")
plotrgl(lighting = FALSE, new=TRUE)
@

\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Conditional estimation}

\begin{exampleblock}{Exercise 1}
  \begin{enumerate}
    \item load \texttt{jumpingdistance.csv}
    \item Use plots and lm() to test whether mass increases jumping distance
  \end{enumerate}
\end{exampleblock}

<<eval=FALSE, echo=FALSE>>=
    set.seed(123)
    height <- rnorm(1000, 1.65, 0.1)
    mass <- 20*(height)^2 + rnorm(1000, 0, 4)
    plot(height, mass, ylim=c(0,100))
    
    jump <- abs(height+2 + 0.5*(height-mean(height))/sd(height) - 0.2*((mass-mean(mass))/sd(mass)) +rnorm(1000))
  plot(mass, jump)
  plot(height, jump)
  summary(lm(jump ~ mass))
  summary(lm(jump ~ mass + height))
  write.csv(data.frame(jump=jump, mass=mass, height=height), file = "jumpingdistance.csv", row.names = FALSE)  
@

<<>>=
  jumping <- read.csv(file = "jumpingdistance.csv")
@

A first approach suggests mass increases jumping distance:
<<eval=FALSE>>=  
    summary(lm(jump ~ mass, data=jumping))
    plot(mass, jump)
@
  
But that is incorrect and due to the correlation between mass and height:
<<eval=FALSE>>=  
    summary(lm(jump ~ mass + height, data=jumping))
@
The direct (causal) effect of mass is negative, as revealed by a multiple regression

\end{frame}
%%%%%%%%%%%

\begin{frame}{Conditional estimation}
  \begin{center}
  \begin{tikzpicture}
  \node (cor) at (-8, 2) {Total / marginal effects};
    \node (jump1) at (-6,0) {jumping distance};
    \node (height1) at (-9, 1) {height};
    \node (mass1) at (-9, -1) {mass};
    \draw[->, thick, color=red] (height1) -- (jump1);
    \draw[->, thick, color=red] (mass1) -- (jump1);

  \draw[dashed] ($(jump1.east)+(0.2,2)$)-- ($(jump1.east)+(0.2,-2)$);
  
  \pause
  
    \node (caus) at (-2, 2) {Direct / conditional effects};
    \node (jump) at (0,0) {jumping distance};
    \node (height) at (-3, 1) {height};
    \node (mass) at (-3, -1) {mass};
    \draw[->, ultra thick, color=red] (height) -- (jump);
    \draw[->, color=blue] (mass) -- (jump);
    \draw[->, color=red, thick] (height)--(mass);
  \end{tikzpicture}
  \end{center}
  
  \uncover<2>{
  \begin{itemize}
    \item Marginal effects $\approx$ raw correlations, sum of direct and indirect effects
    \item Multiple regression estimates direct effects (conditional on other predictors) $\rightarrow$ may reveal causal relationships
  \end{itemize}}
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Conditional estimation}
  <<echo=FALSE, eval=FALSE>>=
  set.seed(123)
  years <- 1960:2015
  births <- round(15+0.8*(years-mean(years))/sd(years) + rnorm(length(years), 0, 1),0)
  storks <- sapply(exp(1+1.5*(years-mean(years))/sd(years) + rnorm(length(years), 0, 0.05)), function(x) rpois(lambda = x, n=1))
  temperature <- 13 + 0.2*(years-mean(years))/sd(years) + rnorm(length(years), 0, 0.2)
  
  plot(years, births)
  plot(years, storks)
  plot(storks, births)
  summary(lm(births ~ storks))
  summary(lm(births ~ storks + years))
  summary(lm(births ~ years))
  summary(lm(births ~ temperature+ years))
  summary(lm(births ~ temperature+storks+ years))
  
  write.csv(x=data.frame(babies_born=births, number_of_storks = storks, mean_temperature=temperature, year=years), file="babies.csv", row.names = FALSE)
  @
  
  \begin{exampleblock}{Exercise 2}
    \begin{enumerate}
      \item Load \texttt{babies.csv}
      \item What drives change in number of babies born?
    \end{enumerate}
  \end{exampleblock}
  
  
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Conditional estimation warning: more covariates is not always better}
  <<echo=FALSE>>=
  set.seed(123)
  innovativeness <- rnorm(1000)
  rigor <- rnorm(1000) + 0.3*innovativeness

  impact <- rigor + innovativeness + rnorm(1000)
  @
  
  \textbf{Are more innovative papers less rigorous?}
  \begin{center}
  \begin{tikzpicture}
    \node (cor) at (-8, 2) {Research question};
    \node (rig) at (-9,-1) {Scientific rigor};
    \node (innov) at (-9, 1) {Innovativeness};
    \draw[->, color=blue] (innov)--(rig);
    \node (qm) at (-8.8,0) {\textbf{?}};
    
    \pause
    \node (impact) at (-6, 0) {Impact};
    \draw[->, color=red, thick] (rig)--(impact);
    \draw[->, color=red, thick] (innov)--(impact);
    
  \end{tikzpicture}
  \end{center}
  
  \textit{Should you correct for publication impact?}
  
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Conditional estimation final warning: more is not always better}
    \textit{Should you include publication impact?}
<<>>=
    summary(lm(rigor ~ innovativeness + impact))$coefficients
  @
  Apparent {\color{blue}{negative}} effect of innovativeness ?

<<>>=
    summary(lm(rigor ~ innovativeness))$coefficients
  @
    Apparent {\color{red}{positive}} effect of innovativeness ?
    
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Conditional estimation final warning: more is not always better}
    \textit{Should you include publication impact?}

  \pause
  Data simulated with positive effect of innovativeness on rigor (simulated slope 0.3)\\ \pause
  \textbf{You should NOT correct for impact}\\ \bigskip \pause
  \textbf{\large \color{red} Rule of Thumb: Do not correct for variables influenced by your predictor outside the causal path of interest}
\end{frame}
%%%%%%%%%%%

\begin{frame}{}

\end{frame}
%%%%%%%%%%%

\begin{frame}{Want more on multiple regression?}

Morrissey \& Ruxton (2018) \textbf{Multiple Regression Is Not Multiple Regressions: The Meaning of Multiple Regression and the Non-Problem of Collinearity}. PTPBIO 10(3)

{\color{blue} \href{dx.doi.org/10.3998/ptpbio.16039257.0010.003}{dx.doi.org/10.3998/ptpbio.16039257.0010.003}}
 
 \begin{quote}
 Simple regression, and multiple regression, typically correspond to very different biological questions. The former use regression lines to describe univariate associations. The latter describe the partial, or direct, effects of multiple variables, conditioned on one another. We suspect that the superficial similarity of simple and multiple regression leads to confusion in their interpretation. [\dots] There is no general sense in which collinearity is a problem. [\dots] Purported solutions to the perceived problems of collinearity are detrimental to most biological analyses.
 \end{quote}
\end{frame}
%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interaction}

\begin{frame}[fragile]{Warnings}

  \begin{alertblock}{Vocabulary warning!}
    \begin{itemize}[<+->]
      \item \textbf{correlation}: linear association between two variables \textit{"how well does $x$ explain $y$ ?"}
      \item \textbf{interaction}: non-additive effect of two or more variables \textit{"does the effect of $x_1$ on $y$ change as a function of $x_2$?"}. Adds a predictor (or several) to a model.
    \end{itemize}
  \end{alertblock}

\pause
  <<histinter, echo=FALSE, dev='tikz', out.height='0.6\\textheight', out.width='0.6\\textwidth'>>=
  setPar()
    a <- 2
    b <- 3
    ab <- 5
    abi <-0.2

    barplot(height = c(a,b,ab), ylab="response", names.arg = c("a", "b", "a+b"))
    barplot(height = c(a,b,abi), add = TRUE, col="red")
    legend(x="topleft", legend = c("No interaction", "Interaction"), fill = c("gray", "red"))
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Fitting an interaction}
  <<echo=FALSE>>=
    set.seed(123)
    x1 <- rnorm(100)
    x2 <- rnorm(100)
    y <- 1 + -0.4*x1 + 0.5*x2 + 0.2*x1*x2 + rnorm(100)
  @

  <<eval=FALSE>>=
    lm(y ~ 1 + x1 * x2)
    lm(y ~ 1 + x1 + x2 + x1:x2)
  @
  \pause
  <<>>=
    summary(lm(y~ 1 + x1*x2))
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Fitting an interaction}
Why the multiplication sign?
\pause
<<>>=
x1Xx2 <- x1*x2
@
\pause
<<>>=
     summary(lm(y~ 1 + x1 + x2 + x1Xx2))
@

\end{frame}
%%%%%%%%%%%


\begin{frame}[fragile]{Warnings}
  \begin{alertblock}{Modeling warning!}
    \begin{itemize}
      \item \sout{DO NOT COMPARE P-VALUES OF TWO MODELS TO TEST FOR AN INTERACTION}
    \end{itemize}
  \end{alertblock}

<<echo=FALSE>>=
    set.seed(123)
  x <- exp(rnorm(200))
  x <- x[order(x)]
  sex <- c(rep(0, 150), rep(1, 50) )
  y <- 1 + sex * 0.0 + x*0.2 + rnorm(200, 0, 0.5)


  masssex <- data.frame(mass = x, sex=sex, movement = y)
@

<<eval=FALSE, echo=FALSE>>=


  write.csv(masssex, file = "../data/masssex.csv", row.names = FALSE)

  summary(lm(movement ~ mass, data=masssex[masssex$sex==0,]))
  summary(lm(movement ~ mass, data=masssex[masssex$sex==1,]))

  summary(lm(movement ~ mass*sex, data=masssex))
@

  \begin{exampleblock}{Exercise}
    \begin{enumerate}
      \item Load the data masssex.csv
      \item Fit a simple regression explaining movement by mass for each sex separately. Is the relationship different between sexes?
      \item Fit the multiple regression explaining movement by mass, sex, and mass:sex, using the full dataset. Is the relationship different between sexes?
      \item Try to understand the discreapancy by plotting the data
    \end{enumerate}
  \end{exampleblock}

\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Warnings}
1.
<<eval=FALSE>>=
  masssex <- read.csv(file="masssex.csv")
@
\pause
2.
<<eval=FALSE>>=
  summary(lm(movement ~ mass, data=masssex[masssex$sex==0,]))
  summary(lm(movement ~ mass, data=masssex[masssex$sex==1,]))
@
\pause
3.
<<eval=FALSE>>=
  summary(lm(movement ~ mass*sex, data=masssex))
@
\end{frame}
%%%%%%%%%%%
\begin{frame}[fragile]{Warnings}
4. Visualize the problem (result on next slide):
<<eval=FALSE>>=
plot(masssex[masssex$sex==0,"mass"],masssex[masssex$sex==0,"movement"],
 col="blue", xlim=range(masssex$mass), ylim=range(masssex$movement),
   xlab="mass", ylab="movement")
points(masssex[masssex$sex==1,"mass"],
       masssex[masssex$sex==1,"movement"], col="red")
legend(x="topleft", col=c("blue", "red"), 
       legend = c("Sex 0", "Sex 1"), pch=1)
@
The slope of movement on mass is the same for both sexes, but the range of values is much smaller for sex 0, so that there is no power to detect a significant effect. Analysing sexes separately is unsound. You must fit an interaction in a model with both sexes to test for an interaction.

\end{frame}
%%%%%%%%%%%
\begin{frame}[fragile]{Warnings}
4.

<<intersexbehav, echo=TRUE,dev='tikz', out.height='0.7\\textheight', out.width='0.7\\textwidth', echo=FALSE>>=
setPar()
     plot(masssex[masssex$sex==0,"mass"],
          masssex[masssex$sex==0,"movement"],
       col="blue", xlim=range(masssex$mass),
       ylim=range(masssex$movement),
       xlab="mass", ylab="movement")
  points(masssex[masssex$sex==1,"mass"],
         masssex[masssex$sex==1,"movement"], col="red")
  legend(x="topleft", col=c("blue", "red"),
         legend = c("Sex 0", "Sex 1"), pch=1)
@
\end{frame}
%%%%%%%%%%%
% 
\begin{frame}[fragile]{Prediction}

<<echo=FALSE, eval=FALSE>>=
  set.seed(123)
  x1 <- rnorm(1000)
  x2 <- 0.5+rnorm(1000) + 0.5*x1
  y <- 10 + 0.1*x1 + 0.2*x2 + 0.5*x1*x2 + rnorm(1000)
  
  write.csv(x = data.frame(plantsize = y, x_location=x1, y_location=x2), file = "plantsize.csv", row.names = FALSE)
  summary(lm(y ~ x1 * x2))
  summary(lm(y ~ x1+ x2))
@
 
  \begin{exampleblock}{Exercise}
    \begin{enumerate}
      \item Load plantsize.csv and plot the data
      \item Fit an additive model explaining plant size by x and y coordinates
      \uncover<2>{\item Create a prediction for plant size as a function of x for two values of y}
    \end{enumerate}
  \end{exampleblock}
  
    <<>>=
    plantsize <- read.csv("plantsize.csv")
    m0 <- lm(plantsize ~ x_location + y_location, data=plantsize)
  @
  
\end{frame}
%%%%%%%%%%


\begin{frame}[fragile]{Prediction}

3.1. Predict
<<>>=
    
  newdata <- data.frame(x_location = rep(seq(-3,3, length.out = 100),2),
                        y_location = c(rep(-3, 100), rep(4,100)))
  newdata$prediction <- predict(m0, newdata = newdata)
@

\end{frame}
%%%%%%%%%%

\begin{frame}[fragile]{Prediction}

3.2 Visualize 
<<eval=FALSE>>=
  plot(newdata$x_location[newdata$y_location==-3],
       newdata$prediction[newdata$y_location==-3],
       xlab="x location", ylab="plant size", type="l",
       ylim = range(newdata$prediction), col="blue")
  lines(newdata$x_location[newdata$y_location==4], 
        newdata$prediction[newdata$y_location==4], col="red")
@
<<predadditive, echo=FALSE,dev='tikz', out.height='0.5\\textheight', out.width='0.5\\textwidth'>>=
  setPar()
  plot(newdata$x_location[newdata$y_location==-3],
       newdata$prediction[newdata$y_location==-3],
       xlab="x location", ylab="plant size", type="l",
       ylim = range(newdata$prediction), col="blue")
  lines(newdata$x_location[newdata$y_location==4], 
        newdata$prediction[newdata$y_location==4], col="red")
@

  
\end{frame}
%%%%%%%%%%

\begin{frame}[fragile]{Prediction with interaction}
  \begin{exampleblock}{Exercise}
    \begin{enumerate}
      \item Load plantsize.csv and plot the data
      \item Fit an additive model explaining plant size by x and y coordinates
     \item Create a prediction for plant size as a function of x for two values of y and plot it
     \item Fit an interaction between x and y coordinates
     \item Create a new prediction with interaction, and plot it
    \end{enumerate}
  \end{exampleblock}
\end{frame}
%%%%%%%%%%

  \begin{frame}[fragile]{Prediction with interaction}

  <<predinteractive, echo=FALSE,dev='tikz', out.height='0.7\\textheight', out.width='0.7\\textwidth'>>=
  setPar()
    m1 <- lm(plantsize ~ x_location * y_location, data=plantsize)
    newdata <- data.frame(x_location = rep(seq(-3,3, length.out = 100),2),
                        y_location = c(rep(-3, 100), rep(4,100)))
  newdata$prediction <- predict(m1, newdata = newdata)
  plot(newdata$x_location[newdata$y_location==-3], newdata$prediction[newdata$y_location==-3],
       xlab="x location", ylab="plant size", type="l", ylim = range(newdata$prediction), col="blue")
  lines(newdata$x_location[newdata$y_location==4], newdata$prediction[newdata$y_location==4], col="red")
@

\end{frame}
%%%%%%%%%%
\begin{frame}[fragile]{Prediction with interaction}
  \begin{exampleblock}{Exercise}
    \begin{enumerate}
      \item Load plantsize.csv and plot the data
      \item Fit an additive model explaining plant size by x and y coordinates
     \item Create a prediction for plant size as a function of x for two values of y and plot it
     \item Fit an interaction between x and y coordinates
     \item Create a new prediction with interaction, and plot it
     \item Compare estimates and p-values across models. Do you think x location has an effect or not?
    \end{enumerate}
  \end{exampleblock}
\end{frame}
%%%%%%%%%%

  \begin{frame}[fragile]{Prediction with interaction}

<<eval=FALSE>>=
m1 <- lm(plantsize ~ x_location * y_location, data=plantsize)
newdata <- data.frame(x_location = rep(seq(-3,3, length.out = 10),10),
                 y_location = rep(seq(-3,4, length.out = 10), each=10))
newdata$prediction <- predict(m1, newdata = newdata)
plot(newdata$x_location[newdata$y_location==-3],
     newdata$prediction[newdata$y_location==-3],
       xlab="x location", ylab="plant size", type="l",
     ylim = range(newdata$prediction), col="blue")
lines(newdata$x_location[newdata$y_location==4], 
      newdata$prediction[newdata$y_location==4], col="red")
@

\end{frame}
%%%%%%%%%%%

  \begin{frame}[fragile]{Prediction with interaction}

<<eval=FALSE>>=
   library(reshape2)
  matpred <- acast(newdata, x_location~y_location, value.var="prediction")
  layout(mat = matrix(data = c(1,2),nrow = 1), widths = c(3,1) )
  image(t(matpred), col = topo.colors(10), xlab = "x location",
        ylab = "y location")
  
  par(mar=c(5, 0,4, 6)+0.1)
  image(matrix(data = seq(min(newdata$prediction),
              max(newdata$prediction), length.out = 10), nrow= 1 ), 
        col=topo.colors(10), xaxt = "n", yaxt="n", main="legend")
  axis(side = 4, at = c(0,0.2,0.4,0.6,0.8,1),
labels = round(seq(min(newdata$prediction), max(newdata$prediction), 
                   length.out =6),1))
@
\end{frame}
%%%%%%%%%%

  \begin{frame}[fragile]{Prediction with interaction}

  <<predinteractivemat, echo=FALSE,dev='tikz', out.height='0.7\\textheight', out.width='0.7\\textwidth'>>=
  setPar()
    m1 <- lm(plantsize ~ x_location * y_location, data=plantsize)
    newdata <- data.frame(x_location = rep(seq(-3,3, length.out = 10),10),
                        y_location = rep(seq(-3,4, length.out = 10), each=10))
  newdata$prediction <- predict(m1, newdata = newdata)
  
  library(reshape2)
  matpred <- acast(newdata, x_location~y_location, value.var="prediction")
  layout(mat = matrix(data = c(1,2),nrow = 1), widths = c(3,1) )
  image(t(matpred), col = topo.colors(10), xlab = "x location",
        ylab = "y location")
  
  par(mar=c(5, 0,4, 6)+0.1)
  image(matrix(data = seq(min(newdata$prediction),
              max(newdata$prediction), length.out = 10), nrow= 1 ), 
        col=topo.colors(10), xaxt = "n", yaxt="n", main="legend")
  axis(side = 4, at = c(0,0.2,0.4,0.6,0.8,1),
labels = round(seq(min(newdata$prediction), max(newdata$prediction), 
                   length.out =6),1))

  @

\end{frame}
%%%%%%%%%%

\end{document}
