\documentclass[12pt]{beamer}%
\setbeamersize{text margin left=0.5cm, text margin right=0.5cm}

\usepackage{alltt}%
\usetheme[background=light]{metropolis} 
\usecolortheme{seahorse}

\usepackage[utf8]{inputenc}%


\usepackage[normalem]{ulem}%strikeout
 

% graphics
%% Figures %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{xcolor}%for color mixing

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}

\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{hyperref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%% Doc info %%%%%%%%%%%%%%%%%%%
\title{Model selection in R, part 2}
\subtitle{Information Criteria and statistical inference}
\author{Timoth\'ee Bonnet}
\institute{BDSI / RSB}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

<<Plot Options, echo=FALSE, message=FALSE>>=
#load(file = ".RData")
library(knitr)
library(xtable)
opts_knit$set(width=60)
opts_chunk$set(comment=NA, fig.width=8, fig.height=6, out.width='0.8\\textwidth',
               out.height='0.6\\textwidth',background='#D7DDEB', size="small")


szgr <- 2
szax <- 1.3
marr <- c(4, 4, 1, 1) + 0.1
setPar<-function(){
par(las=1,mar=marr, cex=szgr, cex.lab=szax , cex.axis=szax, lwd=2 ,pch=1, las=1)
}
setPar()
@

\begin{frame}
  \begin{quote}
  ``Data dredging (also called data snooping, data mining, post hoc data analysis)
should generally be avoided, except in (1) the early stages of exploratory work
or (2) after a more confirmatory analysis has been done. In this latter case,
the investigator should fully admit to the process that led to the post hoc
results and should treat them much more cautiously than those found under
the initial, a priori, approach."
  \end{quote}

Burnham and Anderson, 2002, Model Selection and Multimodel Inference: A Practical Information-theoretic Approach

\end{frame}
%%%%%%%%%%%

\begin{frame}
\maketitle
\end{frame}
%%%%%%%%%%%


\AtBeginSection[]
{
  \begin{frame}<beamer>
    \Large
    \frametitle{}
    \tableofcontents[currentsection,hideothersubsections,subsectionstyle=hide]% down vote\tableofcontents[currentsection,currentsubsection,hideothersubsections,sectionstyle=show/hide,subsectionstyle=show/shaded/hide]
  \end{frame}
}

\begin{frame}[standout]{If you get bored:}
Scroll down the slides and find "Challenge exercises".
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model selection and automation}

\begin{frame}{Reminder: Why model selection}

  \begin{exampleblock}{}
    \begin{itemize}
      \item Adding predictors increases fit to the response, in the current data
      \item But too many predictors:
        \begin{itemize}
          \item DECREASE fit in new data (from the same population)
          \item Hinder biological interpretation
          \item Increases esimtation uncertainty (larger SE and p-values)
        \end{itemize}
      \item Model selection aims to balance fit and generalisation 
    \end{itemize}
  \end{exampleblock}
\end{frame}
%%%%%%%%%%%

\begin{frame}{Information criteria}

\begin{exampleblock}{Akaike information criterion (AIC)}
  \begin{itemize}[<+->]
    \item AIC = 2$\times$Number of parameters - 2$\times \log(\text{model likelihood})$
    \item Smaller is better
    \item Only relative measure, no absolute meaning
    \item In R: \texttt{AIC(model)}
  \end{itemize}
\end{exampleblock}

\end{frame}
%%%%%%%%%%%

\begin{frame}{Practice: reminder AIC}

\begin{center}
  \includegraphics[width=0.4\textwidth]{Figures/vole}
\end{center}

Load \texttt{VoleWeight.csv}. 

We want to understand what factors explain variation in individual body weight. Compare a few (plausible) models with AIC.

\end{frame}
%%%%%%%%%%%

\begin{frame}{Different information criteria?}
\begin{alertblock}{2 most important:}
\begin{enumerate}
  \item AICc (\texttt{MuMIn::AICc()})
    \begin{itemize} 
      \item Small sample size correction for AIC
      \item Can always been used instead of AIC
      \item \textbf{Maximizes prediction} (of new data)
    \end{itemize}
    \pause
  \item Bayesian Information Criterion (\texttt{stat::BIC()})
    \begin{itemize}
      \item More penalty per parameter 
      \item Simpler models than AIC / AICc
      \item \textbf{Maximizes consistency} (=effects you find in model selection data likely to be present in new data)
    \end{itemize}
\end{enumerate}
\end{alertblock}

\end{frame}
%%%%%%%%%%%

\begin{frame}{Package \texttt{MuMIn}}
 
<<eval=FALSE>>=
install.packages("MuMIn")
library(MuMIn)
@

\pause

\begin{exampleblock}{1. AICc}
  \begin{itemize}
    \item AIC is biased for small sample size
    \item AICc (``second-order AIC") when sample size / number of parameters is less than 40
    \item \texttt{MuMIn::AICc()}
  \end{itemize}
\end{exampleblock}

\pause

\begin{alertblock}{2. dredge}
  \begin{itemize}
    \item Automate model selection
    \item Many competing models, some may not make sense
    \item \texttt{MuMIn::dredge()}
  \end{itemize}
\end{alertblock}

\end{frame}
%%%%%%%%%%%

\begin{frame}{Try automated model selection}

Try to use \texttt{dredge()}, with selection based on \texttt{AICc}, to automate model selection on the vole data. Start from a model including all predictors (plus some interactions).

For some reason you first need to run:
<<eval=TRUE>>=
options(na.action="na.fail")
@


Do you find the same result as on slide 6?

\end{frame}
%%%%%%%%%%%

\begin{frame}{dredge() best practices}

<<eval=FALSE>>=
dredge(global.model= ,fixed = ,varying= ,subset= ,rank=)
@

\begin{itemize}[<+->]
  \item \texttt{global.model} makes sense; not too complicated
  \item \texttt{fixed} coefficients across models
  \item \texttt{subset} and \texttt{varying} for more complex subset of models to test
  \item \texttt{rank=AICc} or \texttt{BIC}; decide before you look!
\end{itemize}

\end{frame}
%%%%%%%%%%%


\begin{frame}[fragile]{Practice: constrain dredge}

Still using the vole data, use dredge to select among models of the effects of humidity and temperature while controling for sex and age. Use the arguments \texttt{fixed} or \texttt{varying}. 

Also check the difference between AICc and BIC based model selection.

Your starting model is:
<<eval=FALSE>>=
m0 <- lm(Weight ~ Sex*Age + humidity*temperature +
           as.factor(Year) , data = voles)
@

<<eval=FALSE, echo=FALSE>>=
voles <- read.csv("VoleWeight.csv")
summary(voles)
voles <- voles[!is.na(voles$Weight),]

voles$Body_Length[is.na(voles$Body_Length)] <- round(mean(voles$Body_Length, na.rm = TRUE),digits = 0)
voles$Tail_Length[is.na(voles$Tail_Length)] <- round(mean(voles$Tail_Length, na.rm = TRUE), digits = 0)
write.csv(voles, "VoleWeight.csv", quote = FALSE, row.names = FALSE)


m0 <- lm(Weight ~ Sex*Age + humidity*temperature + as.factor(Year) , data = voles)

resdvole <- dredge(global.model = m0, fixed = ~Sex*Age)
resdvole <- dredge(global.model = m0, fixed = ~Sex*Age, rank = "BIC")

@

\end{frame}
%%%%%%%%%%%



\section{Model selection and causal inference}

\begin{frame}{Principle of model selection based causal inference}
\begin{exampleblock}{}
  \begin{enumerate}[<+->]
    \item List biological hypotheses
    \item Define statistical models corresponding to hypotheses
    \item Importantly, the models do not have to be nested or look the same
    \item (AIC/)BIC model selection
    \item Which models/hypotheses do better? Is one clearly best? 
    \item Do NOT trust parameter estimates/p-values (need confirmatory model)
  \end{enumerate}
\end{exampleblock}
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Practice: Competing hypotheses}
<<echo=FALSE, eval=FALSE>>=
set.seed(123)
nobs <- 350
mass <- abs(rnorm(nobs, 2, 2))
roundness <- runif(n =nobs, 0.2,0.9)
respiration <- (mass^(3/4)) + rnorm(nobs,0,0.2)
metabo <- data.frame(respiration=respiration, mass=mass, roundness=roundness)

write.csv(metabo, file = "metabo.csv", quote = FALSE, row.names = FALSE)

metabo <- read.csv("metabo.csv")
summary(lm(respiration ~ I(mass^(3/4)), data = metabo))
BIC(lm(respiration ~ I(mass^(3/4))+roundness, data = metabo))
BIC(lm(respiration ~ I(mass^(3/4)), data = metabo))
BIC(lm(respiration ~ I(mass^(2/3)), data = metabo))
BIC(lm(respiration ~ I(mass), data = metabo))
BIC(lm(respiration ~ log(mass), data = metabo))
BIC(lm(respiration ~ roundness, data = metabo))
@


How does respiration rate scale with body mass in mammals? For a while researchers fought over different hypotheses: respiration could increase as a function of $mass^{2/3}$, as a function of $mass^{3/4}$ or as a function of $log(mass)$; while maybe the shape of the animals played a role. Let's find out!

Load \texttt{metabo.csv} and compare models through AIC or BIC selection.

NB: you can fit exponents of a predictors using the function \texttt{I()}. For instance, for the exponent 0.5 of x:
<<eval=FALSE>>=
lm(y ~ I(x^(1/2)))
@

\end{frame}
%%%%%%%%%%%


\begin{frame}{Do not choose statistical framework after applying them; do not trust model estimate after model selection}
\begin{figure}
\includegraphics[width=0.5\textwidth]{Figures/dirk-jan-hoek}
\caption{Null-hypothesis testing after model selection \copyright Dirk Jan-Hoek}
\end{figure}
\end{frame}
%%%%%%%%%%%

\begin{frame}{Practice}

Tell what drives the increase in number of babies in \texttt{babies.csv}. Compare AICc vs. BIC model selection. 

<<echo=FALSE, eval=FALSE>>=
babies <- read.csv("babies.csv")
m0 <- lm(babies_born ~ ., data = babies)
dredge(m0)
dredge(m0, rank = "BIC")
summary(lm(babies_born ~ year + number_of_storks, data = babies))
@
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{Practice (challenge): P-values and model selection}
\footnotesize
When a predictor is independent of the response there is a 5\% probability to find a p-value below 0.05 (that's a false positive). But it does not work if we do model selection first! Create a for-loop based on the code below to look at the distribution of p-values after model selection.

<<eval=FALSE>>=
nobs <- 60
mainpredictor <- rnorm(nobs)
control1 <- rnorm(nobs) ; control2 <- rnorm(nobs)
control3 <- rnorm(nobs) ; control4 <- rnorm(nobs)
control5 <- rnorm(nobs) ; response <- rnorm(nobs)
mfull <- lm(response ~ mainpredictor + 
      control1*control2*control3 + control4*control5)
modall <- dredge(mfull, fixed = "mainpredictor")
summary(get.models(modall, 1)[[1]])$coefficients[2,4]
@

<<eval=FALSE, echo=FALSE>>=

options(na.action="na.fail")
effsize <- vector(length = 100)
pval <- vector(length=100)
for (i in 1:100)
{
  nobs <- 60
main_predictor <- rnorm(nobs)
control1 <- rnorm(nobs)
control2 <- rnorm(nobs)
control3 <- rnorm(nobs)
control4 <- rnorm(nobs)
control5 <- rnorm(nobs)
response <- rnorm(nobs)
mfull <- lm(response ~ main_predictor + 
              control1*control2*control3 + control4*control5)

modall <- dredge(mfull, fixed = "main_predictor")

pval[i] <- summary(get.models(modall, 1)[[1]])$coefficients[2,4]
effsize[i] <- summary(get.models(modall, 1)[[1]])$coefficients[2,1]
}
hist(pval)
hist(effsize, breaks = 30)
mean(pval<0.05)
@
\end{frame}
%%%%%%%%%%%


\begin{frame}{Summary: Model selection and inference}

  \begin{exampleblock}{}
    \begin{itemize}
      \item AIC best for exploratory / predictive models
      \item BIC best for robust / consistent models
      \item AIC/BIC alone can be used for causal inference if models are all meaningful competing hypotheses
      \item After AIC/AICc selection p-values are wrong
      \item Parameter estimates, p-values and standard errors MUST be computed on new data (Confirmatory model)
    \end{itemize}
  \end{exampleblock}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{frame}{Challenge!}

What drives bird abundance (ABUND) in \texttt{loyn.csv}?

\end{frame}
%%%%%%%%%%%


\begin{frame}{Challenge!}

\textbf{How do differences in AIC between nested models scale with F-statistic p-values for the extra predictor? Use some simulations to explore the issue.}

\end{frame}
%%%%%%%%%%%

\begin{frame}[standout]{Take-home messages}
\vspace{-0.5cm} \large
\begin{exampleblock}{}
  \begin{enumerate}[<+->] 
    \item Model selection biases estimates/p-values; confirm with new data
    \item Model selection alone can be used for inference; needs careful choice of models
    \item Choose method before analysis (AIC or BIC or confirmatory model?)
  \end{enumerate}
\end{exampleblock}

\end{frame}
%%%%%%%%%%%

\begin{frame}{Want to know more?}

\begin{alertblock}{AIC vs. BIC vs. P-values:}
\begin{itemize}
  \item \textbf{``AIC does everything": }Burnham and Anderson, 2002, Model Selection and Multimodel Inference: A Practical Information-theoretic Approach
  \item \textbf{``Sometimes BIC works better":} Brewer \& al. The relative performance of AIC, AICc and BIC in the presence of unobserved heterogeneity. Methods in Ecology and Evolution. 2016, 7, 679–692.
  \item \textbf{``Your goal matters in the choice between AIC, BIC, p-values...:"} Aho \& al. A graphical framework for model selection criteria and significance tests: Refutation, confirmation and ecology. Methods in Ecology and Evolution. 2017;8:47–56.
\end{itemize}
\end{alertblock}

\end{frame}
%%%%%%%%%%

\begin{frame}[standout]{}
  \begin{exampleblock}{Before you leave:}
  \begin{enumerate}
    \item Write one thing you liked and one you disliked on a sticky note
    \item Presence sheet! (HDR career development framework)
    \item Email address to join the Slack channel 
    \item Past workshops at \url{https://timotheenivalis.github.io/RSB-R-Stats-Biology/} 
  \end{enumerate}
\end{exampleblock}
\end{frame}

\end{document}
